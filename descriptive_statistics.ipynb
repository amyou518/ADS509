{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78862705",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "Assignment 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37227294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b314005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samantharivas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samantharivas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samantharivas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc4e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09442601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization/normalization \n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10ed1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing to 'selftext'\n",
    "df['tokens'] = df['selftext'].apply(lambda x: preprocess_text(x) if isinstance(x, str) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397adaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "numerical_stats = df.describe()\n",
    "token_count_stats = df['tokens'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "905c822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common words\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "fdist = FreqDist(all_tokens)\n",
    "most_common_words = fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5429f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Statistics:\n",
      "        num_comments      upvotes    downvotes  upvotes/subscribers\n",
      "count   3969.000000  3969.000000  3969.000000          3969.000000\n",
      "mean      16.956664    30.929885     1.467804             0.000094\n",
      "std       39.490405    73.258161     4.742290             0.000165\n",
      "min        0.000000     0.000000     0.000000             0.000000\n",
      "25%        0.000000     1.000000     0.000000             0.000014\n",
      "50%        1.000000     2.000000     0.000000             0.000043\n",
      "75%       13.000000    29.591837     0.813953             0.000110\n",
      "max      520.000000   948.979592    60.923077             0.003436\n",
      "\n",
      "Token Count Statistics:\n",
      " count     3971\n",
      "unique    3780\n",
      "top         []\n",
      "freq        11\n",
      "Name: tokens, dtype: object\n",
      "\n",
      "Most Common Words:\n",
      " [('like', 6375), ('feel', 5773), ('know', 3935), ('time', 3270), ('get', 3241), ('want', 3168), ('life', 2973), ('even', 2815), ('thing', 2789), ('year', 2564)]\n"
     ]
    }
   ],
   "source": [
    "# output statistics\n",
    "print(\"Numerical Statistics:\\n\", numerical_stats)\n",
    "print(\"\\nToken Count Statistics:\\n\", token_count_stats)\n",
    "print(\"\\nMost Common Words:\\n\", most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ae513",
   "metadata": {},
   "source": [
    "The descriptive statistics were reviewed separately for each subreddit (r/MentalHealth and r/MentalHealthSupport) rather than in a combined manner. By focusing individually on each subreddit, we can gain deeper insights into specific trends within each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b85d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviwing as seperate df \n",
    "mental_health_support_df = pd.read_csv('data/mental_health_support_posts.csv')\n",
    "mental_health_df = pd.read_csv('data/mental_health_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e3e7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# removing additional stop words\n",
    "#additional_stopwords = {'like', 'feel', 'know', 'get', 'time', 'want', 'life', 'even', 'thing', 'year'}\n",
    "#stop_words = stop_words.union(additional_stopwords)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def analyze_subreddit(subreddit_df):\n",
    "    subreddit_df['tokens'] = subreddit_df['selftext'].apply(lambda x: preprocess_text(x) if isinstance(x, str) else [])\n",
    "    numerical_stats = subreddit_df.describe()\n",
    "    token_count_stats = subreddit_df['tokens'].apply(len).describe()\n",
    "    all_tokens = [token for tokens in subreddit_df['tokens'] for token in tokens]\n",
    "    fdist = FreqDist(all_tokens)\n",
    "    most_common_words = fdist.most_common(10)\n",
    "    return numerical_stats, token_count_stats, most_common_words\n",
    "\n",
    "def analyze_sentiments(subreddit_df):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def get_sentiment_category(sentiment):\n",
    "        if sentiment['compound'] > 0:\n",
    "            return 'positive'\n",
    "        elif sentiment['compound'] < 0:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    # Perform sentiment analysis and store results in new columns\n",
    "    subreddit_df['sentiment'] = subreddit_df['selftext'].apply(lambda x: sid.polarity_scores(x) if isinstance(x, str) else {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0})\n",
    "    \n",
    "    subreddit_df['sentiment_category'] = subreddit_df['sentiment'].apply(get_sentiment_category)\n",
    "    \n",
    "    # Extract compound score for description\n",
    "    sentiment_stats = subreddit_df['sentiment'].apply(lambda x: x['compound']).describe()\n",
    "    \n",
    "    # Count sentiment categories\n",
    "    sentiment_category_counts = subreddit_df['sentiment_category'].value_counts()\n",
    "    \n",
    "    return sentiment_stats, sentiment_category_counts\n",
    "\n",
    "def analyze_sentiments_vader(texts):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiments = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            sentiment = sid.polarity_scores(text)\n",
    "        else:\n",
    "            sentiment = sid.polarity_scores('')\n",
    "        sentiments.append(sentiment)\n",
    "    return sentiments\n",
    "\n",
    "def generate_word_cloud(tokens, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(tokens))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def topic_modeling(tokens_list, num_topics=5):\n",
    "    dictionary = corpora.Dictionary(tokens_list)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    topics = lda_model.print_topics(num_words=10)\n",
    "    return topics\n",
    "\n",
    "def plot_time_series(subreddit_df, title):\n",
    "    subreddit_df.set_index('created_utc', inplace=True)\n",
    "    monthly_counts = subreddit_df.resample('M').size()\n",
    "    monthly_counts.plot(figsize=(10, 5), title=title)\n",
    "    plt.ylabel('Number of Posts')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faee4f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics for r/MentalHealthSupport:\n",
      "Numerical Statistics:\n",
      "        num_comments      upvotes    downvotes  upvotes/subscribers\n",
      "count   1926.000000  1926.000000  1926.000000          1926.000000\n",
      "mean       2.141745     2.972800     0.131159             0.000064\n",
      "std        5.406600     4.882752     0.361374             0.000106\n",
      "min        0.000000     0.000000     0.000000             0.000000\n",
      "25%        0.000000     1.000000     0.000000             0.000022\n",
      "50%        0.000000     2.000000     0.000000             0.000043\n",
      "75%        2.000000     3.000000     0.000000             0.000065\n",
      "max      154.000000   158.585859     3.739130             0.003436\n",
      "\n",
      "Token Count Statistics:\n",
      " count    1928.000000\n",
      "mean      113.366701\n",
      "std       117.371957\n",
      "min         0.000000\n",
      "25%        42.000000\n",
      "50%        78.000000\n",
      "75%       142.000000\n",
      "max      1159.000000\n",
      "Name: tokens, dtype: float64\n",
      "\n",
      "Most Common Words:\n",
      " [('like', 3633), ('feel', 3264), ('know', 2263), ('time', 1910), ('get', 1882), ('want', 1837), ('life', 1702), ('thing', 1627), ('even', 1618), ('really', 1473)]\n",
      "\n",
      "Sentiment Statistics:\n",
      " count    1928.000000\n",
      "mean       -0.223432\n",
      "std         0.794307\n",
      "min        -0.999800\n",
      "25%        -0.956050\n",
      "50%        -0.637200\n",
      "75%         0.707300\n",
      "max         0.999700\n",
      "Name: sentiment, dtype: float64\n",
      "\n",
      "Sentiment Category Counts:\n",
      " negative    1183\n",
      "positive     728\n",
      "neutral       17\n",
      "Name: sentiment_category, dtype: int64\n",
      "\n",
      "Sampled Positive Words:\n",
      "['feel', 'friend', 'later', 'feel', 'february', 'caring', 'disorder', 'found', 'got', 'make', 'familiar', 'used', 'big', 'time', 'enters', 'may', 'explain', 'know', 'alive', 'year']\n",
      "\n",
      "Sampled Neutral Words:\n",
      "['seem', 'half', 'night', 'annoying', 'reddit', 'heavy', 'night', 'break', 'see', 'boss', 'starting', 'youtube', 'long', 'received', 'background', 'recently', 'someone', 'appreciating', 'behavior', 'life']\n",
      "\n",
      "Sampled Negative Words:\n",
      "['little', 'abandonment', 'made', 'thing', 'mother', 'commenced', 'end', 'today', 'drop', 'done', 'mold', 'person', 'wanting', 'episode', 'kind', 'let', 'feel', 'biological', 'stuff', 'tool']\n"
     ]
    }
   ],
   "source": [
    "#'r/MentalHealthSupport' subreddit\n",
    "mental_health_support_numerical_stats, mental_health_support_token_count_stats, mental_health_support_common_words = analyze_subreddit(mental_health_support_df)\n",
    "mental_health_support_sentiment_stats, mental_health_support_sentiment_category_counts = analyze_sentiments(mental_health_support_df)\n",
    "mental_health_support_topics = topic_modeling(mental_health_support_df['tokens'].tolist())\n",
    "\n",
    "# extract words\n",
    "sample_size = 20  \n",
    "\n",
    "positive_words = [token for tokens, sentiment in zip(mental_health_support_df['tokens'], mental_health_support_df['sentiment_category']) if sentiment == 'positive' for token in tokens]\n",
    "neutral_words = [token for tokens, sentiment in zip(mental_health_support_df['tokens'], mental_health_support_df['sentiment_category']) if sentiment == 'neutral' for token in tokens]\n",
    "negative_words = [token for tokens, sentiment in zip(mental_health_support_df['tokens'], mental_health_support_df['sentiment_category']) if sentiment == 'negative' for token in tokens]\n",
    "\n",
    "positive_sample = random.sample(positive_words, min(len(positive_words), sample_size))\n",
    "neutral_sample = random.sample(neutral_words, min(len(neutral_words), sample_size))\n",
    "negative_sample = random.sample(negative_words, min(len(negative_words), sample_size))\n",
    "\n",
    "\n",
    "print(\"\\nDescriptive Statistics for r/MentalHealthSupport:\")\n",
    "print(\"Numerical Statistics:\\n\", mental_health_support_numerical_stats)\n",
    "print(\"\\nToken Count Statistics:\\n\", mental_health_support_token_count_stats)\n",
    "print(\"\\nMost Common Words:\\n\", mental_health_support_common_words)\n",
    "print(\"\\nSentiment Statistics:\\n\", mental_health_support_sentiment_stats)\n",
    "print(\"\\nSentiment Category Counts:\\n\", mental_health_support_sentiment_category_counts)\n",
    "\n",
    "\n",
    "print(\"\\nSampled Positive Words:\")\n",
    "print(positive_sample)\n",
    "\n",
    "print(\"\\nSampled Neutral Words:\")\n",
    "print(neutral_sample)\n",
    "\n",
    "print(\"\\nSampled Negative Words:\")\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3117df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics for r/MentalHealth:\n",
      "Numerical Statistics:\n",
      "              score  upvote_ratio  num_comments      upvotes    downvotes  \\\n",
      "count  2043.000000   2043.000000   2043.000000  2043.000000  2043.000000   \n",
      "mean     54.558003      0.951586     30.923152    57.285904     2.727901   \n",
      "std      90.894256      0.097388     50.996720    94.731162     6.348560   \n",
      "min       0.000000      0.110000      0.000000     0.000000     0.000000   \n",
      "25%       1.000000      0.950000      1.000000     1.000000     0.000000   \n",
      "50%      11.000000      0.990000      8.000000    11.827957     0.010101   \n",
      "75%      69.000000      1.000000     42.000000    72.826087     2.492373   \n",
      "max     930.000000      1.000000    520.000000   948.979592    60.923077   \n",
      "\n",
      "       upvotes/subscribers  \n",
      "count          2043.000000  \n",
      "mean              0.000122  \n",
      "std               0.000202  \n",
      "min               0.000000  \n",
      "25%               0.000002  \n",
      "50%               0.000025  \n",
      "75%               0.000155  \n",
      "max               0.002025  \n",
      "\n",
      "Token Count Statistics:\n",
      " count    2043.000000\n",
      "mean       78.870778\n",
      "std        69.275359\n",
      "min         0.000000\n",
      "25%        31.000000\n",
      "50%        62.000000\n",
      "75%       105.000000\n",
      "max       939.000000\n",
      "Name: tokens, dtype: float64\n",
      "\n",
      "Most Common Words:\n",
      " [('like', 2742), ('feel', 2509), ('know', 1672), ('time', 1360), ('get', 1359), ('want', 1331), ('life', 1271), ('even', 1197), ('thing', 1162), ('people', 1137)]\n",
      "\n",
      "Sentiment Statistics:\n",
      " count    2043.000000\n",
      "mean       -0.171025\n",
      "std         0.770806\n",
      "min        -0.999000\n",
      "25%        -0.926500\n",
      "50%        -0.440400\n",
      "75%         0.697700\n",
      "max         0.997800\n",
      "Name: sentiment, dtype: float64\n",
      "\n",
      "Sentiment Category Counts:\n",
      " negative    1192\n",
      "positive     807\n",
      "neutral       44\n",
      "Name: sentiment_category, dtype: int64\n",
      "\n",
      "Sampled Positive Words:\n",
      "['time', 'guy', 'structure', 'first', 'seeking', 'much', 'nursing', 'like', 'monday', 'open', 'sound', 'thought', 'cut', 'next', 'finding', 'like', 'bond', 'wrong', 'depression', 'year']\n",
      "\n",
      "Sampled Neutral Words:\n",
      "['something', 'access', 'wondering', 'general', 'time', 'info', 'accrington', 'access', 'asking', 'wondering', 'found', 'right', 'post', 'younger', 'straightforward', 'turned', 'asking', 'much', 'rafael', 'always']\n",
      "\n",
      "Sampled Negative Words:\n",
      "['restaurant', 'started', 'completely', 'thing', 'bad', 'everything', 'sound', 'spying', 'like', 'medication', 'month', 'year', 'left', 'texted', 'advice', 'hidden', 'back', 'unfair', 'fear', 'country']\n"
     ]
    }
   ],
   "source": [
    "#'r/MentalHealth' subreddit\n",
    "mental_health_numerical_stats, mental_health_token_count_stats, mental_health_common_words = analyze_subreddit(mental_health_df)\n",
    "mental_health_sentiment_stats, mental_health_sentiment_category_counts = analyze_sentiments(mental_health_df)\n",
    "mental_health_topics = topic_modeling(mental_health_df['tokens'].tolist())\n",
    "\n",
    "# extract words\n",
    "sample_size = 20  \n",
    "\n",
    "positive_words = [token for tokens, sentiment in zip(mental_health_df['tokens'], mental_health_df['sentiment_category']) if sentiment == 'positive' for token in tokens]\n",
    "neutral_words = [token for tokens, sentiment in zip(mental_health_df['tokens'], mental_health_df['sentiment_category']) if sentiment == 'neutral' for token in tokens]\n",
    "negative_words = [token for tokens, sentiment in zip(mental_health_df['tokens'], mental_health_df['sentiment_category']) if sentiment == 'negative' for token in tokens]\n",
    "\n",
    "positive_sample = random.sample(positive_words, min(len(positive_words), sample_size))\n",
    "neutral_sample = random.sample(neutral_words, min(len(neutral_words), sample_size))\n",
    "negative_sample = random.sample(negative_words, min(len(negative_words), sample_size))\n",
    "\n",
    "print(\"\\nDescriptive Statistics for r/MentalHealth:\")\n",
    "print(\"Numerical Statistics:\\n\", mental_health_numerical_stats)\n",
    "print(\"\\nToken Count Statistics:\\n\", mental_health_token_count_stats)\n",
    "print(\"\\nMost Common Words:\\n\", mental_health_common_words)\n",
    "print(\"\\nSentiment Statistics:\\n\", mental_health_sentiment_stats)\n",
    "print(\"\\nSentiment Category Counts:\\n\", mental_health_sentiment_category_counts)\n",
    "\n",
    "print(\"\\nSampled Positive Words:\")\n",
    "print(positive_sample)\n",
    "\n",
    "print(\"\\nSampled Neutral Words:\")\n",
    "print(neutral_sample)\n",
    "\n",
    "print(\"\\nSampled Negative Words:\")\n",
    "print(negative_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
